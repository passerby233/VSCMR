{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word distribution visualize\n",
    "To visualize the word distribution of generated story text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T13:07:38.188453Z",
     "start_time": "2019-04-12T13:07:37.625727Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from rule_mining import utils as ut\n",
    "from rule_mining import config as cfg\n",
    "\n",
    "class TextProcesser(object):\n",
    "    def __init__(self):\n",
    "        if not os.path.exists(cfg.txtdata_file):\n",
    "            get_txtdata()\n",
    "        txtdata = ut.load_json(cfg.txtdata_file)\n",
    "\n",
    "        self.words2id = txtdata['words2id']\n",
    "        self.id2words = txtdata['id2words']\n",
    "        self.words = txtdata['words']\n",
    "        self.word_tag = txtdata['word_tag']\n",
    "        self.disabled_list = txtdata['disabled_list']\n",
    "        self.word_count = txtdata['word_count']\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "\n",
    "    def get_tag(self, word):\n",
    "        if word in self.word_tag:\n",
    "            tag = self.word_tag[word]\n",
    "        else:\n",
    "            tag = nltk.pos_tag([word])[0][1]\n",
    "        return tag\n",
    "\n",
    "    def lemmatize(self, word):\n",
    "        tag = self.get_tag(word)\n",
    "        if tag.startswith('NN'):\n",
    "            lemm = self.wnl.lemmatize(word, pos='n')\n",
    "        elif tag.startswith('VB'):\n",
    "            lemm = self.wnl.lemmatize(word, pos='v')\n",
    "        elif tag.startswith('JJ'):\n",
    "            lemm = self.wnl.lemmatize(word, pos='a')\n",
    "        elif tag.startswith('R'):\n",
    "            lemm = self.wnl.lemmatize(word, pos='r')\n",
    "        else:\n",
    "            lemm = word\n",
    "\n",
    "        return lemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T13:07:38.206128Z",
     "start_time": "2019-04-12T13:07:38.192303Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_word_distr(tp, predictions):\n",
    "    counter = Counter()\n",
    "    print 'len(predictions)=%d' % len(predictions)\n",
    "    total_list = []\n",
    "    for album_id, prediction in predictions.items():\n",
    "        word_list = ut.split_sentence(prediction[0])\n",
    "        for word in word_list:\n",
    "            if word not in tp.disabled_list:\n",
    "                counter.update([word])\n",
    "                if word in tp.words2id:\n",
    "                    total_list.append(tp.words2id[word])\n",
    "\n",
    "    print counter.most_common(30)\n",
    "    data = sorted(counter.values(), reverse=True)[:50]\n",
    "    datasum = sum(data)\n",
    "    data = [value/datasum * 100 for value in data]\n",
    "    plt.figure()\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0, 16])\n",
    "    plt.bar(x=range(len(data)), height=data, width=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T13:07:38.849022Z",
     "start_time": "2019-04-12T13:07:38.211589Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def word_count():\n",
    "    tp = TextProcesser()\n",
    "    \n",
    "    predictions_list = ['/home/lijiacheng/AREL-master/data/save/IRL-XE-ss-init/',\n",
    "                        '/home/lijiacheng/VSCMR/data/save/default/']\n",
    "    for model in predictions_list:\n",
    "        predictions_file = os.path.join(model, \"prediction_test.json\")\n",
    "        if os.path.exists(predictions_file):\n",
    "            predictions = ut.load_json(predictions_file)\n",
    "            ut.printn(\"Processing {}\".format(model))\n",
    "        else:\n",
    "            continue\n",
    "        show_word_distr(tp,predictions)\n",
    "word_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
