{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detector demo\n",
    "based on transactions processed on VIST dataset, unable to predict user input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T13:58:21.093778Z",
     "start_time": "2019-04-12T13:58:12.282202Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('../')\n",
    "from rule_mining import utils as ut\n",
    "from rule_mining import config as cfg\n",
    "from rule_mining.detector import VsDetector\n",
    "\n",
    "im_keep = 10\n",
    "im_range = 2048\n",
    "mode = 'train'\n",
    "\n",
    "trans_info_file = os.path.join(cfg.rule_data_path, \"{}_trans_info.npz\".format(mode))\n",
    "annofile = os.path.join(cfg.dataset_path, \"annotations/sis/{}.story-in-sequence.json\".format(mode))\n",
    "\n",
    "annotations = ut.load_json(annofile)['annotations']\n",
    "trans_info = ut.load_npz_dict(trans_info_file)\n",
    "id2words = ut.load_json(cfg.txtdata_file)['id2words']\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(trans_info['trans_len'], bins=\"auto\")\n",
    "print(len(trans_info['id2trans']))\n",
    "print(\"trans_avg_len = %d\" % trans_info['avg_len'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a detector from a rule_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T13:58:42.215044Z",
     "start_time": "2019-04-12T13:58:21.098355Z"
    }
   },
   "outputs": [],
   "source": [
    "rule_file = os.path.join(cfg.rule_data_path, 'rules03_0.6.npz')\n",
    "detector = VsDetector(rule_file, id2words)\n",
    "print rule_file\n",
    "print \"rules_num = %d, categories_kinds = %d\" % (detector.rules_num, detector.elem_kinds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### show semantic concepts of stories in a range of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T12:52:37.432957Z",
     "start_time": "2019-04-12T12:51:55.138112Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_dir = \"/home/lijiacheng/AREL/VIST/resnet_features/fc/{}/\".format(mode)\n",
    "\n",
    "for i, item in enumerate(annotations[1000:1100]):\n",
    "    if i // 5 % 5 > 0:\n",
    "        continue\n",
    "    annotation = item[0]\n",
    "    img_id = annotation['photo_flickr_id']\n",
    "    text = annotation['text']\n",
    "    \n",
    "    j = i % 5\n",
    "    \n",
    "    if j == 0:\n",
    "        semantics = set()\n",
    "        semantic_list = []\n",
    "        plt.figure(figsize=(25, 150))\n",
    "\n",
    "    img_file = os.path.join(cfg.dataset_path,\"images\",mode,\"{}.jpg\".format(img_id))\n",
    "    if not os.path.exists(img_file):\n",
    "        img_file = os.path.join(cfg.dataset_path,\"images\",mode,\"{}.png\".format(img_id))\n",
    "    try:\n",
    "        img_feature = np.load(os.path.join(feature_dir,\"{}.npy\".format(img_id)))\n",
    "    except:\n",
    "        print(\"feature not found\")\n",
    "        continue\n",
    "    try:\n",
    "        img = Image.open(img_file)\n",
    "    except:\n",
    "        print(\"image %s not found\" % img_id)\n",
    "        continue\n",
    "\n",
    "    plt.subplot(1, 5, 1+j)\n",
    "    plt.imshow(img) \n",
    "    plt.axis('off')\n",
    "    if img_id in trans_info['id2trans']:\n",
    "        im_trans = trans_info['id2trans'][img_id][:10]\n",
    "        semantic_words = detector.detect(im_trans)\n",
    "        semantic_words = [word.encode('utf-8') for word in semantic_words]\n",
    "        semantic_list.append(semantic_words)\n",
    "        semantics.update(semantic_words)\n",
    "        \n",
    "    if j == 4:\n",
    "        plt.show()\n",
    "        #print(semantic_list)\n",
    "        del semantic_list\n",
    "        print(', '.join(list(semantics)))\n",
    "        print '-'*100\n",
    "        del semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### # watch single picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T12:52:37.434325Z",
     "start_time": "2019-04-12T12:51:26.133Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for img_id, trans in trans_info['id2trans'].items()[40:60]:\n",
    "    img_file = os.path.join(cfg.dataset_path,\"images/\",mode,\"{}.jpg\".format(img_id))\n",
    "    try:\n",
    "        img = Image.open(img_file)\n",
    "    except:\n",
    "        print(\"image %s not found\" % img_id)\n",
    "        continue\n",
    "    plt.figure(str(img_id))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(img_id)\n",
    "    plt.show()\n",
    "    \n",
    "    gts = 'gts: '+', '.join([id2words[str(token-im_range)] for token in trans[10:]])\n",
    "    infer = \"inference: \" + ', '.join(detector.detect(trans[:10])) \n",
    "    print gts\n",
    "    print '\\n'\n",
    "    print infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
